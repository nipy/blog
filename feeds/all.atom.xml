<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>The Nipy blog</title><link href="http://blog.nipy.org/" rel="alternate"></link><link href="http://blog.nipy.org/feeds/all.atom.xml" rel="self"></link><id>http://blog.nipy.org/</id><updated>2013-05-27T12:00:00-07:00</updated><entry><title>A separation too early</title><link href="http://blog.nipy.org/science-joins-software.html" rel="alternate"></link><updated>2013-05-27T12:00:00-07:00</updated><author><name>Matthew Brett</name></author><id>tag:blog.nipy.org,2013-05-27:science-joins-software.html</id><summary type="html">&lt;p&gt;I followed a Google+ link to an article that has just come out in Science
magazine. The article is "Troubling Trends in Scientific
Software Use" by Lucas Joppa and others &lt;sup id="fnref:joppa"&gt;&lt;a class="footnote-ref" href="#fn:joppa" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I did not find Joppa's article very satisfying, but there were some
interesting references.  For example (from Joppa &lt;em&gt;et al&lt;/em&gt;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From fields such as high-performance computing, we learn key insights and
best practices for how to develop, standardize, and implement software (11)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reference 11 is an article by Victor Basili and others on "Understanding the
High-Performance-Computing Community" &lt;sup id="fnref:basili"&gt;&lt;a class="footnote-ref" href="#fn:basili" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;. It is a thoughtful reflection
on the experience of academic software engineers working with scientists using
massively parallel computing. The message I took from the article was that
scientists may have good reasons to reject suggestions from academic software
engineers.  This is often because the solutions are too general to be useful
for a particular problem.  The last sentence from the article is "We've much
to learn from each other".&lt;/p&gt;
&lt;p&gt;Another couple of sentences in the Joppa article led me to interesting places:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reliance on personal recommendations and trust is a strategy with risks to
science and scientist. "End-user developers" commonly create scientific
software (17, 21, 22) &lt;sup id="fnref:joppa"&gt;&lt;a class="footnote-ref" href="#fn:joppa" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reference (22) is "A Software Chasm: Software Engineering and Scientific
Computing" by Diane Kelley &lt;sup id="fnref:kelly"&gt;&lt;a class="footnote-ref" href="#fn:kelly" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;. Kelley also has some interesting thoughts
on the separation of science and software engineering:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Creating the chasm&lt;/p&gt;
&lt;p&gt;In a 1997 paper, Iris Vessey described a shift in computing philosophy that
occurred in the late 1960s. She quoted George E. Forsyth, the ACM's president
at that time, as stating that computer science was a field of its own,
separate from the application domains. The result, as Vessey points out, was
the insistence that anyone who wanted to be taken seriously in the field of
computing, and software engineering, must develop domain-independent
techniques, methods, and paradigms. The pressure was such that claims of
broad applicability became commonplace. So, the bridge between software
engineering and any application domain became a single massive structure that
everyone had to use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the relevant quote from Vessey (1997)&lt;sup id="fnref:vessey"&gt;&lt;a class="footnote-ref" href="#fn:vessey" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Then followed a debate, which endured for over a decade, that pitted
academia against the computing profession. The notions of traditional
computer scientists steeped in the notion of theory and the stigma of all
things "applied" led to the following statement by George E. Forsyth,
President of ACM, in 1965: "... the core of computer science has become
and will remain a field of its own, ahead of, and separate from the
application domain specialists." This philosophy led, not only to the
creation of generic "solutions" to problems, but also to claims that any
"creation" was applicable in all situations, because to state otherwise
would have branded the creator as lacking in academic respectability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I wonder if this separation has had a larger effect on our thinking than we
recognize.&lt;/p&gt;
&lt;p&gt;For example, our original NIPY grant application asked for salaries for two
full-time programmers to help us apply software engineering methods to writing
neuroimaging software.  We did hire two programmers but we had given them an
impossible job.  To do a good job, they had to now become experts in a
scientific field they did not know. Useful code and documentation had to be
written so that other scientists in the field would find it easy to follow.
To do this, you need to know the field.  We scientists spend more time than we
think working out how to talk to our colleagues. That experience is very hard
to teach.&lt;/p&gt;
&lt;p&gt;These articles gave me a new way to think about the false separation of
"software" and "science". It is mysterious that we make this separation,
because my whole experience tells me they are closely linked. I wonder whether
we make this separation because we have come under the unconscious influence
of the movement that Kelley and Vessey describe.  It is easy to see how
tempting it is.   How easy life would be if we really could pay a programmer
to write up our ideas as we sketch them airily on a white-board and retire to
the garden, to think great thoughts.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:joppa"&gt;
&lt;p&gt;Joppa, Lucas N., et al. "Troubling Trends in Scientific Software
Use." Science 340.6134 (2013): 814-815.&amp;#160;&lt;a class="footnote-backref" href="#fnref:joppa" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:basili"&gt;
&lt;p&gt;Basili, Victor R., et al. "Understanding the
High-Performance-Computing Community." (2008).&amp;#160;&lt;a class="footnote-backref" href="#fnref:basili" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:kelly"&gt;
&lt;p&gt;Kelly, Diane F. "A software chasm: Software engineering and
scientific computing." Software, IEEE 24.6 (2007): 120-119.&amp;#160;&lt;a class="footnote-backref" href="#fnref:kelly" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:vessey"&gt;
&lt;p&gt;Vessey, Iris. "Problems versus solutions: the role of the application
domain in software." Papers presented at the seventh workshop on Empirical
studies of programmers. ACM, 1997.&amp;#160;&lt;a class="footnote-backref" href="#fnref:vessey" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="scientific software"></category><category term="software engineering"></category><category term="programming"></category><category term="code"></category></entry><entry><title>Unscientific programming</title><link href="http://blog.nipy.org/unscientific-programming.html" rel="alternate"></link><updated>2013-05-26T15:30:00-07:00</updated><author><name>Matthew Brett</name></author><id>tag:blog.nipy.org,2013-05-26:unscientific-programming.html</id><summary type="html">&lt;p&gt;We various of the Berkeley NIPY crowd were in a Mexican restaurant yesterday
lunchtime. We discussed the IPython notebook and scientific programming and
reproducible research.&lt;/p&gt;
&lt;p&gt;One question struck me as fundamental: &lt;em&gt;do scientists have to learn to be
software engineers?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I think the common answer to this is "No - scientific coding is different".
The "No" camp points out that scientists use code in a different way to
software engineers.  Science is exploration, and so is scientific coding.
Scientific code is provisional, often changing.  There is no specfication,
there is no production system that must &lt;em&gt;just work&lt;/em&gt;.  We are trying stuff out,
seeing what works, adjusting to our better understanding of the data and the
ideas.&lt;/p&gt;
&lt;p&gt;Maybe the scientist's idea of the canonical software engineer is someone who
writes and hosts a web application with some database behind it.&lt;/p&gt;
&lt;p&gt;Accepting the "No", we look at things that software engineers should learn:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Version control.  Essential for a software engineer, apparently, because
   they need to be able to rollback their changes, and keep track of released
   and development versions.  Desirable maybe for a scientist, but not
   essential, because the code never goes into production, and we always use
   the latest version of the code.&lt;/li&gt;
&lt;li&gt;Testing.  Essential for a software engineer, otherwise they may release
   code that corrupts a database of financial transactions or delivers the
   wrong item to your address.  Desirable maybe for a scientist, but not
   essential because the nature of the code changes so often that it would
   only slow things down to write exhaustive tests, only for the code to
   change track and make the tests obsolete.&lt;/li&gt;
&lt;li&gt;Code review.  Essential for a software engineer, because they work in teams
   with other software engineers.  This is their job, to write code, and they
   can learn from each other. Desirable maybe for a scientist, but not
   essential because each scientist owns their own problem and so only they
   can understand their code.  Explaining the code is time-consuming and slows
   progress in developing new ideas.  Others in the lab are not software
   engineers, so they are not trained to review code, they will not enjoy it
   and they will not do a good job.&lt;/li&gt;
&lt;li&gt;Releases.  Essential for a software engineer because they may be paid to
   provide code that others can use.  The release labels code that can be
   used.  The software engineer can and should make sure the code works as
   advertised.  Desirable maybe for a scientist, but not essential, because
   the code changes often, and the code may be written to solve a very
   specific problem that could not be of much interest to another researcher.
   If the other researcher is interested, they should make the effort to work
   out how to use the code, that is not the job of the author-scientist, they
   are not paid to support software, but to produce science.&lt;/li&gt;
&lt;li&gt;Documentation.  Essential for a software engineer because they want their
   code to be used correctly by other people.  Desirable but not essential for
   the scientist because scientific code is not for distribution except to
   other scientists who must take responsibility for the code themselves.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think that these set of beliefs lie at the heart of the problem for
reproducible science.&lt;/p&gt;
&lt;p&gt;The beliefs rest on the following model of writing scientific code:&lt;/p&gt;
&lt;h2&gt;A folk model of scientific code&lt;/h2&gt;
&lt;p&gt;A scientist named A writes some code.  They check the code.  They confirm it
is working.  Most likely this means the code is free of major errors.&lt;/p&gt;
&lt;p&gt;Another scientist named B is reading some results published by A. They can
assume that the results are free of major errors.&lt;/p&gt;
&lt;h2&gt;Living with the folk model&lt;/h2&gt;
&lt;p&gt;Now everything makes sense.  Version control, testing, code review, releases,
documentation are great, of course, but if we can assume there are no major
errors, then - why would B want to see my code?  If B gets my code, why would
she need to understand it?  Why would she need to know what version it was, or
how that related to my paper?  She should assume that there are no major
errors.&lt;/p&gt;
&lt;h2&gt;Rejecting the folk model&lt;/h2&gt;
&lt;p&gt;The folk model is not just too simple, it is flat-out wrong.  We &lt;a href="http://blog.nipy.org/ubiquity-of-error.html"&gt;make
mistakes&lt;/a&gt; all the time.  &lt;em&gt;All the time&lt;/em&gt;.  If
you know that, then you know that B needs to check your stuff.  They can't do
science without checking your stuff.  If they need to check your stuff, you
need to write your code &lt;em&gt;for production&lt;/em&gt;.  Then, nothing is optional.  We all
need; version control, testing, code review, releases, documentation.&lt;/p&gt;
&lt;!-- vim:ft=markdown
--&gt;</summary><category term="scientific software"></category><category term="software"></category><category term="programming"></category><category term="code"></category></entry><entry><title>The ubiquity of error</title><link href="http://blog.nipy.org/ubiquity-of-error.html" rel="alternate"></link><updated>2013-05-26T15:00:00-07:00</updated><author><name>Matthew Brett</name></author><id>tag:blog.nipy.org,2013-05-26:ubiquity-of-error.html</id><summary type="html">&lt;p&gt;I was talking to an esteemed colleague about six months ago about how easy it
was to believe that we do not make errors, unless we check.&lt;/p&gt;
&lt;p&gt;He said that he had noticed this change when his students and research
assistants started to use their own computer programs to analyze data.  Before
this, when he and others added up a column of numbers, they would check
several times that they were correct.  After, the student or RA would analyze
some data with a program they had written themselves, and my friend would say
"are you sure the program is right" and they would say "yes I'm sure". My
friend would then go through the results and check; they would often find
mistakes.  He thought that there was something about using a computer that
made it easy to believe that the result was right, even if the process of
getting the result had the same chance of error as a simpler task like adding
numbers.&lt;/p&gt;
&lt;p&gt;To quote David Donoho:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In my own experience, error is ubiquitous in scientific computing, and one needs to work very diligently and energetically to eliminate it. One needs a very clear idea of what has been done in order to know where to look for likely sources of error. I often cannot really be sure what a student or colleague has done from his/her own presentation, and in fact often his/her description does not agree with my own understanding of what has been done, once I look carefully at the scripts. Actually, I find that researchers quite generally forget what they have done and misrepresent their computations.&lt;/p&gt;
&lt;p&gt;Computing results are now being presented in a very loose, “breezy” way—in journal articles, in conferences, and in books. All too often one simply takes computations at face value. This is spectacularly against the evidence of my own experience. I would much rather that at talks and in referee reports, the possibility of such error were seriously examined.&lt;/p&gt;
&lt;p&gt;-- &lt;cite&gt;David L. Donoho (2010). An invitation to reproducible computational
research. Biostatistics Volume 11, Issue 3 Pp. 385-388&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is something about computational science that seems to make the idea of
error less worrying or important:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In stark contrast to the sciences relying on deduction or empiricism,
computational science is far less visibly concerned with the ubiquity of
error. At conferences and in publications, it’s now completely acceptable for
a researcher to simply say, “here is what I did, and here are my results.”
Presenters devote almost no time to explaining why the audience should believe
that they found and corrected errors in their computations. The presentation’s
core isn’t about the struggle to root out error — as it would be in mature
fields — but is instead a sales pitch: an enthusiastic presentation of ideas
and a breezy demo of an implementation. Computational science has nothing like
the elaborate mechanisms of formal proof in mathematics or meta-analysis in
empirical science. Many users of scientific computing aren’t even trying to
follow a systematic, rigorous discipline that would in principle allow others
to verify the claims they make. How dare we imagine that computational
science, as routinely practiced, is reliable!&lt;/p&gt;
&lt;p&gt;-- &lt;cite&gt; David L. Donoho, Arian Maleki, Inam Ur Rahman, Morteza Shahram and
Victoria Stodden (2009) Reproducible Research in Computational Harmonic
Analysis.  Computing in Science and Engineering 11(1) pp 8-18&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="scientific software"></category><category term="software"></category><category term="programming"></category><category term="code"></category><category term="reproducible research"></category></entry><entry><title>Nipy World blog moved from there to here</title><link href="http://blog.nipy.org/nipy-moved.html" rel="alternate"></link><updated>2013-05-12T19:42:00-07:00</updated><author><name>Matthew Brett</name></author><id>tag:blog.nipy.org,2013-05-12:nipy-moved.html</id><summary type="html">&lt;p&gt;The Nipy World blog used to be on Blogspot at &lt;a href="http://nipyworld.blogspot.com"&gt;http://nipyworld.blogspot.com&lt;/a&gt;.
I'm restarting the blog here because it's easier and more fun to write the blog
using &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt;.  I hope that it will be easier for
my fellow NIPistas to add their own posts using Github.&lt;/p&gt;
&lt;!-- vim:ft=markdown
--&gt;</summary><category term="Nipy"></category><category term="Nipyworld"></category><category term="Blog"></category></entry></feed>